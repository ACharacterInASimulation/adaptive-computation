model:
  sequence_len: 512
  n_head: 8
  n_kv_head: 8
  n_embd: 512
  n_layer: 6
  use_adaptive_computation: false
  use_pondernet: true
  n_layers_per_block: 1
  max_pondering_steps: 5
  geom_lambda_prior: 0.5
  kl_weight_beta: 0.05
  cdf_early_stop: 0.999


data:
  dataset_name: "BabyLM"  
  data_dir: "./data/babylm/train_100M/"
  val_data_dir: "./data/babylm/dev/"
  tokenizer_path: "checkpoints"
  shuffle: true


optimizer:
  unembedding_lr: 0.001
  embedding_lr: 0.001
  matrix_lr: 0.001
  halting_lr: 0.04
  weight_decay: 0.0


training:
  batch_size: 16
  num_workers: 4
  steps: 50000
  log_every: 10
  eval_every: 10
  eval_n_samples: 16
  warmup_steps: 50
  gradient_clip: 1.0
  seed: 42


wandb:
  enabled: true
  project: "DL-babylm"
  entity: null  
  name: null  
  tags: ["pondernet", "babylm", "language-modeling"]
  notes: "DL-project (babylm dataset with PonderNet)"


checkpoint:
  save_dir: "checkpoints/babylm"
  save_final: true
  final_name: "babylm_ponder_model.pt"
