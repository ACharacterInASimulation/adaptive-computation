model:
  sequence_len: 512
  n_head: 8
  n_kv_head: 8
  n_embd: 512
  n_layer: 6
  
  use_adaptive_computation: true
  n_layers_per_block: 1
  max_pondering_steps: 4
  act_threshold: 0.99
  halting_penalty: 0.001


data:
  dataset_name: "BabyLM"  # Can be "BabyLM" or "AddMul"
  data_dir: "./data/babylm/train_100M/"
  val_data_dir: "./data/babylm/dev/"  # Validation data directory
  tokenizer_path: "checkpoints"
  shuffle: true


optimizer:
  unembedding_lr: 0.003
  embedding_lr: 0.003
  matrix_lr: 0.003
  halting_lr: 0.06
  weight_decay: 0.01


training:
  batch_size: 32
  num_workers: 4
  steps: 50000
  log_every: 100
  eval_every: 5000
  eval_n_samples: 500
  warmup_steps: 2000
  gradient_clip: 1.0
  seed: 42


wandb:
  enabled: true
  project: "babylm-act"
  entity: null  
  name: null  
  tags: ["act", "babylm", "language-modeling"]
  notes: "ACT training on BabyLM dataset"


checkpoint:
  save_dir: "checkpoints/babylm"
  save_final: true
  final_name: "babylm_act_model.pt"
