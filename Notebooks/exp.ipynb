{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e747cead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "PROJECT_DIR = Path.cwd().parent\n",
    "sys.path.append(str(PROJECT_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "006acf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dataset.synthetic_tasks import AddMul, Difficulty, make_collate_fn, Tokenizer\n",
    "from models.transformer import GPT, TransformerBlockConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "661e3807",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AddMul(num_samples=10, max_operands=4, max_digits=3, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba3d071f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['-', '4', '+', '2', '+', '7', '0', '4', '+', '1', '3', '='], ['7', '1', '5'])\n"
     ]
    }
   ],
   "source": [
    "for data in dataset:\n",
    "    print(data)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ece420c",
   "metadata": {},
   "outputs": [],
   "source": [
    "difficulty  = Difficulty(max_operands=2, max_digits=1, operations=['*'])\n",
    "dataset.set_difficulty(difficulty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4399735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['9', '*', '-', '4', '='], ['-', '3', '6'])\n"
     ]
    }
   ],
   "source": [
    "for data in dataset:\n",
    "    print(data)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4498e516",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(42)\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "max_seq_len = 128\n",
    "\n",
    "ds = AddMul(num_samples=50_000_000, max_operands=6, max_digits=6, seed=0)\n",
    "collate_fn = make_collate_fn(tokenizer, max_seq_len)\n",
    "dl = DataLoader(ds, batch_size=64, shuffle=False, num_workers=0,\n",
    "                collate_fn=collate_fn, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "40db45da",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = TransformerBlockConfig(\n",
    "    sequence_len=max_seq_len,\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_head=4,\n",
    "    n_kv_head=4,\n",
    "    n_embd=128,\n",
    "    n_layer=2,\n",
    "    use_adaptive_computation=True,\n",
    "    n_layers_per_block=2,\n",
    "    max_pondering_steps=5,\n",
    "    act_threshold=0.99,\n",
    "    halting_penalty=0.01,\n",
    ")\n",
    "model = GPT(cfg).to(device)\n",
    "model.init_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0161858d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(16, 128)\n",
       "    (h): ModuleList(\n",
       "      (0): AdaptiveBlock(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Block(\n",
       "            (attn): CausalSelfAttention(\n",
       "              (c_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (c_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (c_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (c_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "            )\n",
       "            (mlp): MLP(\n",
       "              (c_fc): Linear(in_features=128, out_features=512, bias=False)\n",
       "              (c_proj): Linear(in_features=512, out_features=128, bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (halting_unit): HaltingUnit(\n",
       "          (halting_linear): Linear(in_features=128, out_features=1, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=128, out_features=16, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0edf8718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling the LR for the AdamW parameters ∝1/√(128/768) = 2.449490\n"
     ]
    }
   ],
   "source": [
    "opt = model.setup_optimizers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a84288d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5276, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for batch in dl:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "        loss = model(**batch)\n",
    "    print(loss)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5218b010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., device='cuda:0')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.last_expected_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e6e690fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bbabdf0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0007, device='cuda:0')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.h[0].halting_unit.halting_linear.weight.grad.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "74b02821",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e2a21d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c01e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "#torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440adfb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3605caac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 58])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['idx'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "79608262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 58])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['targets'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "70ae1e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 58])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['ponder_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ac8d5a2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.h[0].layers[0].attn.c_q.weight.data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b3f3b61a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(16, 128)\n",
       "    (h): ModuleList(\n",
       "      (0): AdaptiveBlock(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Block(\n",
       "            (attn): CausalSelfAttention(\n",
       "              (c_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (c_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (c_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (c_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "            )\n",
       "            (mlp): MLP(\n",
       "              (c_fc): Linear(in_features=128, out_features=512, bias=False)\n",
       "              (c_proj): Linear(in_features=512, out_features=128, bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (halting_unit): HaltingUnit(\n",
       "          (halting_linear): Linear(in_features=128, out_features=1, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=128, out_features=16, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950bdded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step    50  loss=2.7754  ponder_cost=0.282  exp_steps=2.460  act_penalty=0.0028\n",
      "step   100  loss=2.7755  ponder_cost=0.290  exp_steps=2.445  act_penalty=0.0029\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m log_every = \u001b[32m50\u001b[39m\n\u001b[32m      3\u001b[39m model.train()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mbreak\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/code-lab/lib/python3.12/site-packages/torch/utils/data/dataloader.py:734\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    740\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/code-lab/lib/python3.12/site-packages/torch/utils/data/dataloader.py:790\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    789\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    791\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    792\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/code-lab/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/ACharacterInASimulation/adaptive-computation/dataset/synthetic_tasks.py:112\u001b[39m, in \u001b[36mmake_collate_fn.<locals>.collate\u001b[39m\u001b[34m(batch)\u001b[39m\n\u001b[32m    110\u001b[39m     idx[b, :L] = torch.tensor(Xs[b], dtype=torch.long)\n\u001b[32m    111\u001b[39m     targets[b, :L] = torch.tensor(Ys[b][:L], dtype=torch.long)  \n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     ponder_mask[b, :L] = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mL\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33midx\u001b[39m\u001b[33m\"\u001b[39m: idx, \u001b[33m\"\u001b[39m\u001b[33mtargets\u001b[39m\u001b[33m\"\u001b[39m: targets, \u001b[33m\"\u001b[39m\u001b[33mponder_mask\u001b[39m\u001b[33m\"\u001b[39m: ponder_mask}\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "steps = 20000\n",
    "log_every = 50\n",
    "model.train()\n",
    "for step, batch in enumerate(dl, start=1):\n",
    "    if step > steps:\n",
    "        break\n",
    "    idx = batch[\"idx\"].to(device, non_blocking=True)\n",
    "    targets = batch[\"targets\"].to(device, non_blocking=True)\n",
    "    ponder_mask = batch[\"ponder_mask\"].to(device, non_blocking=True)\n",
    "\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "        loss = model(idx, targets=targets, kv_cache=None,\n",
    "                    loss_reduction=\"mean\", ponder_mask=ponder_mask)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "\n",
    "        if step % log_every == 0:\n",
    "            pc = getattr(model, \"last_ponder_cost\", float(\"nan\"))\n",
    "            es = getattr(model, \"last_expected_steps\", float(\"nan\"))\n",
    "            ap = getattr(model, \"last_act_penalty\", float(\"nan\"))\n",
    "            print(f\"step {step:5d}  loss={loss.item():.4f}  ponder_cost={pc:.3f}  \"\n",
    "                f\"exp_steps={es:.3f}  act_penalty={ap:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b4ebb91c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.6659, device='cuda:0')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.h[0].halting_unit.halting_linear.weight.grad.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ce914ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00315"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.01 * 0.315"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "42defda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem: -965393*93*69*-28= Answer: 173457952668\n",
      "Model output: <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "        input_seq, output_seq = ds[0]\n",
    "        print(\"Problem:\", \"\".join(input_seq), \"Answer:\", \"\".join(output_seq))\n",
    "        prompt_ids = tokenizer.encode(input_seq)  # includes '='\n",
    "        outs = []\n",
    "        for tok in model.generate(tokens=prompt_ids, max_tokens=40, temperature=0.0):\n",
    "            outs.append(tok)\n",
    "            if tok == tokenizer.eos_id:\n",
    "                break\n",
    "        print(\"Model output:\", \"\".join(tokenizer.decode(outs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b7478e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
